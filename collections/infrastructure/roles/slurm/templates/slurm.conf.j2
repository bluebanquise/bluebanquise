#jinja2: lstrip_blocks: True
#### Blue Banquise file ####
## {{ansible_managed}}

# Documentation:
# https://slurm.schedmd.com/slurm.conf.html

## Very minimal slurm.conf

## Controller
ClusterName={{ slurm_cluster_name }}
SlurmctldAddr={{ slurm_controller_address | default(slurm_controller_hostname, true) | default(slurm_control_machine, true) }}
SlurmctldHost={{ slurm_controller_hostname | default(slurm_control_machine, true) }}

{% if slurm_computes_config == "configless" %}
## Use configless mode
SlurmctldParameters=enable_configless
{% endif %}

## Authentication
SlurmUser=slurm
AuthType=auth/munge

## Ports
SrunPortRange={{ slurm_srunportrange }}

## Files path
StateSaveLocation={{ slurm_statesavelocation }}
SlurmdSpoolDir={{ slurm_slurmdspooldir }}
SlurmctldPidFile={{ slurm_slurmctldpidfile }}
SlurmdPidFile={{ slurm_slurmdpidfile }}

## Logging
SlurmctldDebug={{ slurm_slurmctlddebug }}
SlurmctldLogFile={{ slurm_slurmctldlogfile }}
SlurmdDebug={{ slurm_slurmddebug }}
SlurmdLogFile={{ slurm_slurmdlogfile }}

## Timeouts
SlurmctldTimeout={{ slurm_slurmctldtimeout }}
SlurmdTimeout={{ slurm_slurmdtimeout }}

## We don't want a node to go back in pool without sys admin acknowledgement
ReturnToService={{ slurm_returntoservice }}

## Using pmi/pmi2/pmix interface for MPI
MpiDefault={{ slurm_mpidefault | default('',true) }}

## Basic scheduling based on nodes
SchedulerType={{ slurm_schedulertype }}
SelectType={{ slurm_selecttype }}

{% if slurm_grestypes is defined and slurm_grestypes is not none %}
# Gres configuration
GresTypes={{ slurm_grestypes }}
ProctrackType=proctrack/cgroup
TaskPlugin=affinity,cgroup
SelectTypeParameters=CR_Core_Memory,CR_CORE_DEFAULT_DIST_BLOCK,CR_ONE_TASK_PER_CORE

{% endif %}
{% if slurm_enable_accounting %}
## Accounting
AccountingStorageHost={{ slurm_controller_hostname | default(slurm_control_machine, true) }}
AccountingStorageType=accounting_storage/slurmdbd
JobAcctGatherType=jobacct_gather/linux
{% endif %}

## Acct Gather
AcctGatherNodeFreq={{ slurm_acct_gather_node_freq }}
AcctGatherEnergyType=acct_gather_energy/{{ slurm_acct_gather_energy_type }}
AcctGatherInterconnectType=acct_gather_interconnect/{{slurm_acct_gather_interconnect_type}}
AcctGatherFilesystemType=acct_gather_filesystem/{{ slurm_acct_gather_filesystem_type }}
AcctGatherProfileType=acct_gather_profile/{{ slurm_acct_gather_profile_type }}

## Additional content if exist
{% if slurm_slurm_conf_additional_content is defined and slurm_slurm_conf_additional_content is iterable %}
  {% for slurm_slurm_conf_content in slurm_slurm_conf_additional_content %}
{{ slurm_slurm_conf_content | default('', true) }}                                                                                                                       
  {% endfor %}
{% endif %}

## Nodes and partitions list
# Note: in configless mode, only slurm.conf is shipped to slurmds.
# We need to compact all configuration into this single file.
# -> no includes allowed.

{# NODENAMES #}
{# First we create a full list of all nodes that should be included in the file #}
{% set slurm_nodes = [] %}
{% for partition in slurm_partitions_list %}
  {% if partition['computes_groups'] is defined and partition['computes_groups'] is not none %}
    {% for compute_group in partition['computes_groups'] %}
      {% for nn in groups[compute_group] %}{# We cannot merge both list inside a for loop in Jinja2 #}
{{ slurm_nodes.append(nn) }}
      {% endfor %}
    {% endfor %}
  {% endif %}
{% endfor %}
{% set slurm_nodes = (slurm_nodes | unique) %}
{# Now we use bb_nodes_profiles previously cached to pack these nodes per hardware groups #}
{%- set hw_packs = {} -%}
{% if bb_nodes_profiles is mapping %}
  {% for node in slurm_nodes %}
    {% if bb_nodes_profiles[node] is defined and bb_nodes_profiles[node]['hw'] is defined and bb_nodes_profiles[node]['hw'] is not none %}
      {% if bb_nodes_profiles[node]['hw'] not in hw_packs %}
        {% do hw_packs.update({bb_nodes_profiles[node]['hw']: {'nodes': [], 'hw_specs': (hostvars[node]['hw_specs'] | default(none, true)) }}) %}
      {% endif %}
{{ hw_packs[bb_nodes_profiles[node]['hw']]['nodes'].append(node) }}
    {% else %}
# WARNING: node {{ node }} is associated with an hw and an os profile
    {% endif %}
  {% endfor %}
{% endif %}
{# Now we can generate final NodeNames for slurm #}
{% for hw_pack, hw_pack_vars in hw_packs.items() %}
  {% if hw_pack_vars['hw_specs'] is defined and hw_pack_vars['hw_specs'] is not none %}
    {% set buffer = hw_pack_vars['hw_specs'] %}
    {% if buffer.cpu.sockets is defined and buffer.cpu.sockets is not none and buffer.cpu.cores_per_socket is defined and buffer.cpu.cores_per_socket is not none and buffer.cpu.threads_per_core is defined and buffer.cpu.threads_per_core is not none%}
      {% set nodes_parameters = ' Sockets=' + (buffer.cpu.sockets|string) + ' CoresPerSocket=' + (buffer.cpu.cores_per_socket|string) + ' ThreadsPerCore=' + (buffer.cpu.threads_per_core|string) %}
    {% else %}{# Will fail if cores is also not set. Expected minimal value. #}
      {% set nodes_parameters = ' Procs=' + (buffer.cpu.cores | string) %}
    {% endif %}
    {% if buffer.memory is defined and buffer.memory is not none %}
        {% set nodes_parameters = nodes_parameters + ' RealMemory=' + (buffer.memory|string) %}
    {% endif %}
    {% if buffer.slurm_extra_nodes_parameters is defined and buffer.slurm_extra_nodes_parameters is not none %}
        {% set nodes_parameters = nodes_parameters + ' ' +  buffer.slurm_extra_nodes_parameters %}
    {% endif %}
NodeName={{ hw_pack_vars['nodes'] | bluebanquise.infrastructure.nodeset }} {{ nodes_parameters }}
  {% else %}
# WARNING: pack {{ hw_pack }} does not have hw_specs
  {% endif %}
{% endfor %}

{# PARTITIONS #}
## Partitions list
{% if slurm_partitions_list is defined and slurm_partitions_list %}
  {% for partition in slurm_partitions_list %}
    {% set partition_nodes = [] %}
    {% if partition['computes_groups'] is defined and partition['computes_groups'] is not none %}
      {% for compute_group in partition['computes_groups'] %}
        {% for nn in groups[compute_group] %}{# We cannot merge both list inside a for loop in Jinja2 #}
{{ partition_nodes.append(nn) }}
        {% endfor %}
      {% endfor %}
    {% endif %}
    {% set partition_nodes = (partition_nodes | unique) %}
PartitionName={{partition.partition_name}} {% for arg, arg_value in partition.partition_configuration.items() %} {{ arg }}={{ arg_value }}{% endfor %} Nodes={{ partition_nodes | bluebanquise.infrastructure.nodeset }}

  {% endfor %}
{% endif %}
