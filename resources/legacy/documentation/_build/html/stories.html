

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>13. Stories &mdash; BlueBanquise Documentation 1.5.0 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="14. Roles list" href="roles.html" />
    <link rel="prev" title="12. Containers" href="containers.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> BlueBanquise Documentation
          

          
            
            <img src="_static/logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                1.5
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="vocabulary.html">2. Vocabulary</a></li>
<li class="toctree-l1"><a class="reference internal" href="training_sysadmin.html">3. [Training] - Cluster SysAdmin</a></li>
<li class="toctree-l1"><a class="reference internal" href="training_ansible.html">4. [Training] - Ansible</a></li>
<li class="toctree-l1"><a class="reference internal" href="bootstrap.html">5. [Core] - Bootstrap base system</a></li>
<li class="toctree-l1"><a class="reference internal" href="configure_bluebanquise.html">6. [Core] - Configure BlueBanquise</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy_bluebanquise.html">7. [Core] - Deploy BlueBanquise</a></li>
<li class="toctree-l1"><a class="reference internal" href="multiple_icebergs.html">8. [Core] - Manage multiple icebergs</a></li>
<li class="toctree-l1"><a class="reference internal" href="diskless.html">9. [Core] - Diskless</a></li>
<li class="toctree-l1"><a class="reference internal" href="high_availability.html">10. [Community] - High Availability</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitoring.html">11. [Community] - Monitoring</a></li>
<li class="toctree-l1"><a class="reference internal" href="containers.html">12. Containers</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">13. Stories</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#management-and-main-configuration">13.1. Management and main configuration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#versioning-the-inventory">13.1.1. Versioning the inventory</a></li>
<li class="toctree-l3"><a class="reference internal" href="#adding-a-new-repository">13.1.2. Adding a new repository</a></li>
<li class="toctree-l3"><a class="reference internal" href="#update-management-nodes-configuration">13.1.3. Update management nodes configuration</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#nodes">13.2. Nodes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#adding-a-new-node">13.2.1. Adding a new node</a></li>
<li class="toctree-l3"><a class="reference internal" href="#adding-a-new-range-of-nodes">13.2.2. Adding a new range of nodes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#adding-a-new-master-group">13.2.3. Adding a new master group</a></li>
<li class="toctree-l3"><a class="reference internal" href="#adding-a-new-equipment-profile-group">13.2.4. Adding a new equipment_profile group</a></li>
<li class="toctree-l3"><a class="reference internal" href="#adding-a-custom-group">13.2.5. Adding a custom group</a></li>
<li class="toctree-l3"><a class="reference internal" href="#replacing-updating-a-node">13.2.6. Replacing/Updating a node</a></li>
<li class="toctree-l3"><a class="reference internal" href="#deploying-nodes">13.2.7. Deploying nodes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#apply-or-update-nodes-configuration">13.2.8. Apply or update nodes configuration</a></li>
<li class="toctree-l3"><a class="reference internal" href="#changing-equipment-profile-group-of-some-nodes">13.2.9. Changing equipment_profile group of some nodes</a></li>
<li class="toctree-l3"><a class="reference internal" href="#manage-multiple-distribution-versions">13.2.10. Manage multiple distribution versions</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#roles-and-playbooks">13.3. Roles and playbooks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#create-a-custom-role">13.3.1. Create a custom role</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#security">13.4. Security</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#update-root-password">13.4.1. Update root password</a></li>
<li class="toctree-l3"><a class="reference internal" href="#use-vault-to-enhance-inventory-security">13.4.2. Use vault to enhance inventory security</a></li>
<li class="toctree-l3"><a class="reference internal" href="#externalize-ansible">13.4.3. Externalize Ansible</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#storage">13.5. Storage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#lvm-mirroring">13.5.1. LVM mirroring</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#policies">13.5.1.1. Policies</a></li>
<li class="toctree-l4"><a class="reference internal" href="#create-volume-groupe">13.5.1.2. Create Volume Groupe</a></li>
<li class="toctree-l4"><a class="reference internal" href="#create-logical-volume">13.5.1.3. Create Logical Volume</a></li>
<li class="toctree-l4"><a class="reference internal" href="#format-and-mount-the-volume">13.5.1.4. Format and mount the volume</a></li>
<li class="toctree-l4"><a class="reference internal" href="#add-a-mirror">13.5.1.5. Add a mirror</a></li>
<li class="toctree-l4"><a class="reference internal" href="#recover-from-crash">13.5.1.6. Recover from crash</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="roles.html">14. Roles list</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">15. References</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">BlueBanquise Documentation</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li><span class="section-number">13. </span>Stories</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/stories.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="stories">
<h1><span class="section-number">13. </span>Stories<a class="headerlink" href="#stories" title="Permalink to this headline">¶</a></h1>
<p>This part contains few stories to help you face common tasks / issues seen
while using the stack.</p>
<div class="section" id="management-and-main-configuration">
<h2><span class="section-number">13.1. </span>Management and main configuration<a class="headerlink" href="#management-and-main-configuration" title="Permalink to this headline">¶</a></h2>
<div class="section" id="versioning-the-inventory">
<h3><span class="section-number">13.1.1. </span>Versioning the inventory<a class="headerlink" href="#versioning-the-inventory" title="Permalink to this headline">¶</a></h3>
<p>It is a good practice to version the whole /etc/bluebanquise/inventory/
folder, to keep track of all changes made, and be able to revert in case of
issues.</p>
<p>It may even be better to use an external git platform (gitlab,
etc.) to backup the whole configuration, and be able to push changes from the
cluster to this platform.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Never use a public repository (like github) to store your inventory, as it
contains sensitive data about your infrastructure.</p>
</div>
<p>To create a basic git repository, assuming you already have an existing
configuration in /etc/bluebanquise/inventory/, do:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>cd /etc/bluebanquise/inventory/
git init
git add --all
git commit --author=&quot;my_name &lt;my_name@my_mail.org&gt;&quot; -m &quot;First commit&quot;
</pre></div>
</div>
<p>It is now possible to commit new changes after each configuration change.</p>
<p>You can find a lot on the web about correct git repository usage, creating
branches, etc.</p>
<p>Few useful commands:</p>
<ul class="simple">
<li><p><cite>git log</cite>: display recent logs of the current repository</p></li>
<li><p><cite>git reset –soft HEAD~1</cite>: undo the last commit</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If used, it may be also a good idea to version the
/etc/bluebanquise/playbooks/ and /etc/bluebanquise/roles/custom/ folders.</p>
</div>
</div>
<div class="section" id="adding-a-new-repository">
<h3><span class="section-number">13.1.2. </span>Adding a new repository<a class="headerlink" href="#adding-a-new-repository" title="Permalink to this headline">¶</a></h3>
<p>To add a new repository, simply create its root folder (if not
existing), using the distribution to be used, the major distribution version
associated, and the architecture:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mkdir /var/www/html/repositories/centos/8/x86_64/
</pre></div>
</div>
<p>Then create your new repository folder:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mkdir /var/www/html/repositories/centos/8/x86_64/my_repo
</pre></div>
</div>
<p>And add it to your global repositories, into file
/etc/bluebanquise/inventory/group_vars/all/general_settings/repositories.yml or
if this repository is only associated with a specific equipment group, add it
into the repositories.yml associated with the equipment_profile associated, for
example in file
/etc/bluebanquise/inventory/group_vars/equipment_X/repositories.yml .</p>
<p>If needed, it is also possible to define minor version based repositories. To
do so, simply replace the major version by the minor version, and use the
<cite>ep_operating_system.distribution_version</cite> variable to force usage of a minor
distribution version for the equipment groups associated with this new
repository.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mkdir /var/www/html/repositories/centos/8.2/x86_64/
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[root@management1 ~]# cat /etc/bluebanquise/inventory/group_vars/equipment_X/equipment_profile.yml | grep distribution
  distribution: centos
  distribution_major_version: 8
  distribution_version: 8.2
[root@management1 ~]#
</pre></div>
</div>
<p>It is also possible to add an environment associated to a repository. To do so,
add the environment name after the <em>repositories</em> folder. Assuming here we wish
to create a new <em>production</em> repository:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mkdir /var/www/html/repositories/production/centos/8.2/x86_64/

.. code-block:: text

  [root@management1 ~]# cat /etc/bluebanquise/inventory/group_vars/equipment_X/equipment_profile.yml | grep -E &#39;distribution|environment&#39;
    distribution: centos
    distribution_major_version: 8
    repositories_environment: production
  [root@management1 ~]#
</pre></div>
</div>
</div>
<div class="section" id="update-management-nodes-configuration">
<span id="id1"></span><h3><span class="section-number">13.1.3. </span>Update management nodes configuration<a class="headerlink" href="#update-management-nodes-configuration" title="Permalink to this headline">¶</a></h3>
<p>After each change in the inventory, it is needed to update the management nodes
configuration (and sometime even all nodes configurations if the change has a
wide impact).</p>
<p>To update the main configuration, simply re-run your management playbook.
However, few tips:</p>
<ol class="arabic simple">
<li><p>Do a dry run first before running a playbook on a management node.</p></li>
</ol>
<p>To do so, use the <cite>–check –diff</cite> arguments with the <cite>ansible-playbook</cite>
command.</p>
<p>For example:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ansible-playbook /etc/bluebanquise/playbooks/managements.yml --diff --check
</pre></div>
</div>
<p>2. If you have multiple managements nodes, update them one after the other, using
the <cite>–limit</cite> argument, specifying the nodes each time. This can be combined
with the dry run seen in 1.</p>
<p>For example:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ansible-playbook /etc/bluebanquise/playbooks/managements.yml --limit management1
ansible-playbook /etc/bluebanquise/playbooks/managements.yml --limit management2
ansible-playbook /etc/bluebanquise/playbooks/managements.yml --limit management3
</pre></div>
</div>
</div>
</div>
<div class="section" id="nodes">
<h2><span class="section-number">13.2. </span>Nodes<a class="headerlink" href="#nodes" title="Permalink to this headline">¶</a></h2>
<div class="section" id="adding-a-new-node">
<span id="id2"></span><h3><span class="section-number">13.2.1. </span>Adding a new node<a class="headerlink" href="#adding-a-new-node" title="Permalink to this headline">¶</a></h3>
<p>To add a new node, go into the /etc/bluebanquise/inventory/cluster/nodes folder.</p>
<p>Then here, find the file related to the master group of the new node. If you
need to create a new master group, refer to the related story bellow.</p>
<p>Open the file, and find the equipment_profile related to this node. If you
need to create a new equipment_profile group, refer to the related story bellow.</p>
<p>Now, simply add the node under the equipment_profile group, under <em>hosts</em>:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">mg_computes</span><span class="p">:</span>
  <span class="nt">children</span><span class="p">:</span>
    <span class="nt">equipment_typeC</span><span class="p">:</span>
      <span class="nt">hosts</span><span class="p">:</span>
        <span class="nt">c001</span><span class="p">:</span>  <span class="l l-Scalar l-Scalar-Plain">&lt;&lt;&lt;&lt; my new node</span>
</pre></div>
</div>
<p>You may also wish to add some network to the node. To do so, add a
network_interfaces list this way.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">mg_computes</span><span class="p">:</span>
  <span class="nt">children</span><span class="p">:</span>
    <span class="nt">equipment_typeC</span><span class="p">:</span>
      <span class="nt">hosts</span><span class="p">:</span>
        <span class="nt">c001</span><span class="p">:</span>
          <span class="nt">bmc</span><span class="p">:</span>                        <span class="c1"># This instruction defines an attached BMC</span>
            <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">bmanagement1</span>        <span class="c1"># This is the hostname of the BMC</span>
            <span class="nt">ip4</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">10.10.3.1</span>            <span class="c1"># This is the ipv4 of the BMC</span>
            <span class="nt">mac</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">08:00:27:dc:f8:f6</span>    <span class="c1"># This is the MAC hardware address of the BMC (for DHCP)</span>
            <span class="nt">network</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">ice1-1</span>           <span class="c1"># This is the logical network this interface is connected to. Logical networks will be seen later.</span>
          <span class="nt">network_interfaces</span><span class="p">:</span>         <span class="c1"># This is an instruction, to define bellow all host&#39;s NIC (Network Interface Controllers)</span>
            <span class="p p-Indicator">-</span> <span class="nt">interface</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">enp0s3</span>       <span class="c1"># This is the NIC name (&#39;ip a&#39; command to get NIC list)</span>
              <span class="nt">ip4</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">10.10.3.1</span>          <span class="c1"># This is the expected ipv4 for this NIC</span>
              <span class="nt">mac</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">08:00:27:dc:f8:f5</span>  <span class="c1"># This is the NIC MAC address, for the DHCP</span>
              <span class="nt">network</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">ice1-1</span>         <span class="c1"># This is the logical network this NIC is linked to</span>
            <span class="p p-Indicator">-</span> <span class="nt">interface</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">ib0</span>          <span class="c1"># This is another NIC, not in the dhcp so no MAC is provided</span>
              <span class="nt">ip4</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">10.20.3.1</span>
              <span class="nt">network</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">interconnect-1</span>
</pre></div>
</div>
<p>Then use the ansible-inventory command to check that the new host is listed on
the configuration and seen by Ansible:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[root@management1 ~]# ansible-inventory --graph
@all:
  |--@mg_computes:
  |  |--@equipment_typeC:
  |  |  |--c001
  |--@mg_logins:
  |  |--@equipment_typeL:
  |  |  |--login1
  |--@mg_managements:
  |  |--@equipment_typeM:
  |  |  |--management1
  |--@ungrouped:
[root@management1 ~]#
</pre></div>
</div>
<p>Now, since we added a new node, replay the playbooks on management nodes (see
<a class="reference internal" href="#update-management-nodes-configuration"><span class="std std-ref">Update management nodes configuration</span></a>) and if you are using the
hosts_file role on all the cluster nodes, also replay their playbook, maybe
limiting the execution to the needed roles, using tags. For example:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ansible-playbook /etc/bluebanquise/playbooks/computes.yml -t hosts_file
</pre></div>
</div>
</div>
<div class="section" id="adding-a-new-range-of-nodes">
<h3><span class="section-number">13.2.2. </span>Adding a new range of nodes<a class="headerlink" href="#adding-a-new-range-of-nodes" title="Permalink to this headline">¶</a></h3>
<p>You may need to add a new range of nodes. You can do it manually, adding them
one by one, or simply use a small bash script to generate the content you need.</p>
<p>We assume here you need to generate a new range of c[1-4] of nodes, with ip
on range 10.10.3.[1-4]. Same kind of ranges for their BMC and interconnect.</p>
<p>Create a file /root/gen.sh with the following content:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>#!/bin/bash
cat &lt;&lt;EOF &gt; computes.yml
mg_computes:
  children:
    equipment_typeC:
      hosts:
EOF
for ((i=1;i&lt;=$1;i++)); do
cat &lt;&lt;EOF &gt;&gt; computes.yml
        c$i:
          bmc:
            name: bc$i
            ip4: 10.10.103.$i
            mac:
            network: ice1-1
          network_interfaces:
            - interface: enp0s9
              ip4: 10.10.3.$i
              mac:
              network: ice1-1
            - interface: ib0
              ip4: 10.20.3.$i
              network: interconnect-1
EOF
done
</pre></div>
</div>
<p>Save, make this script executable, and run it asking for 4 nodes:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>chmod +x /root/gen.sh
/root/gen.sh 4
</pre></div>
</div>
<p>You should now have a file named <em>computes.yml</em> inside your current folder with
the desired content. Refer <a class="reference internal" href="#adding-a-new-node"><span class="std std-ref">Adding a new node</span></a> and
<a class="reference internal" href="#update-management-nodes-configuration"><span class="std std-ref">Update management nodes configuration</span></a> stories seen above on how now
update the cluster configuration.</p>
</div>
<div class="section" id="adding-a-new-master-group">
<h3><span class="section-number">13.2.3. </span>Adding a new master group<a class="headerlink" href="#adding-a-new-master-group" title="Permalink to this headline">¶</a></h3>
<p>You may need to create a new master group, for a new kind of range of equipment.</p>
<p>The stack is fully dynamic regarding groups. The only thing you need is to
create a new file with the master group name inside of
/etc/bluebanquise/inventory/cluster/nodes/</p>
<p>For example, if you wish to create a new group “switches”, create file
/etc/bluebanquise/inventory/cluster/nodes/switches.yml and add the following
content in the file:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">mg_switches</span><span class="p">:</span>
  <span class="nt">children</span><span class="p">:</span>
</pre></div>
</div>
<p>The master group is now created.</p>
<p>Note that master groups must always be prefixed by the string <em>mg_</em> to be
detected by the stack. It is also possible for advanced users to change this
prefix pattern in the general_settings part.</p>
</div>
<div class="section" id="adding-a-new-equipment-profile-group">
<h3><span class="section-number">13.2.4. </span>Adding a new equipment_profile group<a class="headerlink" href="#adding-a-new-equipment-profile-group" title="Permalink to this headline">¶</a></h3>
<p>To create a new equipment profile, create its associated folder. We will assume
here that you wish to create equipment profile equipment_X:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mkdir /etc/bluebanquise/inventory/group_vars/equipment_X
</pre></div>
</div>
<p>Then, if this equipment need to be different than the generic equipment_profile
configuration (/etc/bluebanquise/inventory/group_vars/all/equipment_all/),
create new files into /etc/bluebanquise/inventory/group_vars/equipment_X and use
Ansible precedence mechanism to set your settings.</p>
<p>You can refer to the example inventories in resources/examples/ to see more of
these files.</p>
<p>You can now add nodes into this equipment profile. See <a class="reference internal" href="#adding-a-new-node"><span class="std std-ref">Adding a new node</span></a>.</p>
</div>
<div class="section" id="adding-a-custom-group">
<h3><span class="section-number">13.2.5. </span>Adding a custom group<a class="headerlink" href="#adding-a-custom-group" title="Permalink to this headline">¶</a></h3>
<p>You can add custom groups in the stack (for your own convenience). To do so, go
into folder /etc/bluebanquise/inventory/cluster/groups/ .
Here, create a new file, called for example <em>mygroup</em>, with the following
content:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[my_group]
c[001:004]
login1

[my_group:vars]
color=yellow
</pre></div>
</div>
<p>You ca now see that the group was created, using <cite>ansible-inventory –graph</cite>
command.</p>
<p>Also note that all variables defined here (this is not a YAML file, so we use
and = to define variables here) are provided to members of <cite>my_group</cite>.</p>
</div>
<div class="section" id="replacing-updating-a-node">
<h3><span class="section-number">13.2.6. </span>Replacing/Updating a node<a class="headerlink" href="#replacing-updating-a-node" title="Permalink to this headline">¶</a></h3>
<p>When a node fail, you may need to replace it. This means updating its MAC
address and provision/deploy it again.</p>
<p>To do so, edit the file that contains the node, for example
/etc/bluebanquise/inventory/cluster/nodes/computes.yml and simply update the MAC
address.</p>
<p>Then update the dhcp configuration on the management node:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>ansible-playbook /etc/bluebanquise/playbooks/managements.yml -t dhcp_server
</pre></div>
</div>
<p>The service should already have restarted since an Ansible handler do it when
some configuration files are updated.</p>
<p>Now ensure you can ping the BMC of the new node (if BMC there is).</p>
<p>Ask for a new deployment using bootset (see <a class="reference internal" href="#deploying-nodes"><span class="std std-ref">Deploying nodes</span></a>).</p>
</div>
<div class="section" id="deploying-nodes">
<span id="id3"></span><h3><span class="section-number">13.2.7. </span>Deploying nodes<a class="headerlink" href="#deploying-nodes" title="Permalink to this headline">¶</a></h3>
<p>To deploy or redeploy a node, use the bootset tool. We will assume here we need
to deploy node c001.</p>
<p>First check bootset status of the node:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[root@management1 ]# bootset -n c001 -s
[INFO] Loading /etc/bluebanquise/pxe/nodes_parameters.yml
[INFO] Loading /etc/bluebanquise/pxe/pxe_parameters.yml
Diskfull: c001
[root@management1 ]#
</pre></div>
</div>
<p>Node is set to boot on disk (or maybe nothing if this is the first time node is
used).</p>
<p>As for an os deployment, using:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>bootset -n c001 -b osdploy
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>bootset accept nodeset ranges, or clustershell groups.</p>
</div>
<p>And check again:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[root@management1 ]# bootset -n c001 -s
[INFO] Loading /etc/bluebanquise/pxe/nodes_parameters.yml
[INFO] Loading /etc/bluebanquise/pxe/pxe_parameters.yml
Next boot deployment: c001
[root@management1 ]#
</pre></div>
</div>
<p>Now boot/reboot the target node, and have it boot over PXE.</p>
<p>You can check the process on the node screen/console, but also by monitoring
logs and the bootset tool.</p>
<p>In a first shell, launch:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>journalctl -u dhcpd -u atftpd -f
</pre></div>
</div>
<p>This will monitor the dhcp and the tftp servers (first couple to dialog with the deploying node).</p>
<p>In a second shell launch:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>tail -f /var/log/httpd/*
</pre></div>
</div>
<p>This will monitor all the http (apache2) requests: the iPXE chain, and the
kernel/initrd and packages download.</p>
<p>In a last shell, launch:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>watch -n 10 bootset -n c001 -q -s
</pre></div>
</div>
<p>You will now be able to follow the whole deployment process, steps by steps.</p>
</div>
<div class="section" id="apply-or-update-nodes-configuration">
<h3><span class="section-number">13.2.8. </span>Apply or update nodes configuration<a class="headerlink" href="#apply-or-update-nodes-configuration" title="Permalink to this headline">¶</a></h3>
<p>To be done.</p>
</div>
<div class="section" id="changing-equipment-profile-group-of-some-nodes">
<h3><span class="section-number">13.2.9. </span>Changing equipment_profile group of some nodes<a class="headerlink" href="#changing-equipment-profile-group-of-some-nodes" title="Permalink to this headline">¶</a></h3>
<p>To be done.</p>
</div>
<div class="section" id="manage-multiple-distribution-versions">
<h3><span class="section-number">13.2.10. </span>Manage multiple distribution versions<a class="headerlink" href="#manage-multiple-distribution-versions" title="Permalink to this headline">¶</a></h3>
<p>Allows to boot group of nodes with different distributions versions (major or
minor), and use different kernel on each group.</p>
<p>To be done.</p>
</div>
</div>
<div class="section" id="roles-and-playbooks">
<h2><span class="section-number">13.3. </span>Roles and playbooks<a class="headerlink" href="#roles-and-playbooks" title="Permalink to this headline">¶</a></h2>
<div class="section" id="create-a-custom-role">
<h3><span class="section-number">13.3.1. </span>Create a custom role<a class="headerlink" href="#create-a-custom-role" title="Permalink to this headline">¶</a></h3>
<p>To be done.</p>
</div>
</div>
<div class="section" id="security">
<h2><span class="section-number">13.4. </span>Security<a class="headerlink" href="#security" title="Permalink to this headline">¶</a></h2>
<div class="section" id="update-root-password">
<h3><span class="section-number">13.4.1. </span>Update root password<a class="headerlink" href="#update-root-password" title="Permalink to this headline">¶</a></h3>
<p>To be done.</p>
</div>
<div class="section" id="use-vault-to-enhance-inventory-security">
<h3><span class="section-number">13.4.2. </span>Use vault to enhance inventory security<a class="headerlink" href="#use-vault-to-enhance-inventory-security" title="Permalink to this headline">¶</a></h3>
<p>To be done.</p>
</div>
<div class="section" id="externalize-ansible">
<h3><span class="section-number">13.4.3. </span>Externalize Ansible<a class="headerlink" href="#externalize-ansible" title="Permalink to this headline">¶</a></h3>
<p>To be done.</p>
</div>
</div>
<div class="section" id="storage">
<h2><span class="section-number">13.5. </span>Storage<a class="headerlink" href="#storage" title="Permalink to this headline">¶</a></h2>
<div class="section" id="lvm-mirroring">
<h3><span class="section-number">13.5.1. </span>LVM mirroring<a class="headerlink" href="#lvm-mirroring" title="Permalink to this headline">¶</a></h3>
<p>This story explains how to create mirrored volumes using
the stack or manually, and how to recover in case of crash.
It is recommended, before launching production, to try a
crash scenario and recover, to ensure procedure work.</p>
<p>Interesting command:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>lsblk -o name,mountpoint,label,size,uuid
</pre></div>
</div>
<p>This story explains how to create a 2 mirrors LV, then
extend it to 3 mirrors, and recover from a crash.</p>
<div class="section" id="policies">
<h4><span class="section-number">13.5.1.1. </span>Policies<a class="headerlink" href="#policies" title="Permalink to this headline">¶</a></h4>
<p>First, set policies to <em>remove</em>, for both
<strong>mirror_log_fault_policy</strong> and <strong>mirror_image_fault_policy</strong> in <em>lvm.conf</em>.
Nothing automatic should occur now. Reboot the system to ensure all is stable.</p>
</div>
<div class="section" id="create-volume-groupe">
<h4><span class="section-number">13.5.1.2. </span>Create Volume Groupe<a class="headerlink" href="#create-volume-groupe" title="Permalink to this headline">¶</a></h4>
<p>We assume here there are 3 physical disks to be used:</p>
<ul class="simple">
<li><p>/dev/sdb1</p></li>
<li><p>/dev/sdc1</p></li>
<li><p>/dev/sdd1</p></li>
</ul>
<div class="section" id="using-the-stack">
<h5><span class="section-number">13.5.1.2.1. </span>Using the stack<a class="headerlink" href="#using-the-stack" title="Permalink to this headline">¶</a></h5>
<p>Inside your hosts definition (here mngt1-1), set the following parameters:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">hosts</span><span class="p">:</span>
  <span class="nt">mngt1-1</span><span class="p">:</span>
    <span class="nt">storage</span><span class="p">:</span>
      <span class="nt">lvm</span><span class="p">:</span>
        <span class="p p-Indicator">-</span> <span class="nt">vg</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">vg1</span>
          <span class="nt">pvs</span><span class="p">:</span>
            <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">/dev/sdc1</span>
            <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">/dev/sdb1</span>
            <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">/dev/sdd1</span>
</pre></div>
</div>
<p>And execute the <em>lvm</em> role on this host.</p>
</div>
<div class="section" id="manually">
<h5><span class="section-number">13.5.1.2.2. </span>Manually<a class="headerlink" href="#manually" title="Permalink to this headline">¶</a></h5>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[root@mngt1-1 ~]# pvcreate /dev/sdc1
[root@mngt1-1 ~]# pvcreate /dev/sdb1
[root@mngt1-1 ~]# pvcreate /dev/sdd1

[root@mngt1-1 ~]# pvdisplay
--- Physical volume ---
PV Name               /dev/sdb1
VG Name               vg1
PV Size               1023.00 MiB / not usable 3.00 MiB
Allocatable           yes
PE Size               4.00 MiB
Total PE              255
Free PE               255
Allocated PE          0
PV UUID               wc7uAA-VCMc-uL2P-oQv1-o1kd-uxz1-eQWk0v

--- Physical volume ---
PV Name               /dev/sdc1
VG Name               vg1
PV Size               1023.00 MiB / not usable 3.00 MiB
Allocatable           yes
PE Size               4.00 MiB
Total PE              255
Free PE               255
Allocated PE          0
PV UUID               derqgB-sGVQ-1hQT-Ryuo-grIN-CNID-BxRBvr

--- Physical volume ---
PV Name               /dev/sdd1
VG Name               vg1
PV Size               1023.00 MiB / not usable 3.00 MiB
Allocatable           yes
PE Size               4.00 MiB
Total PE              255
Free PE               255
Allocated PE          0
PV UUID               J4WnOi-ssJm-yRDA-A2MM-wakz-04Rg-OdMTU2

[root@mngt1-1 ~]# vgcreate vg1 /dev/sdb1 /dev/sdc1 /dev/sdd1

[root@mngt1-1 ~]# vgdisplay
--- Volume group ---
VG Name               vg1
System ID
Format                lvm2
Metadata Areas        3
Metadata Sequence No  1
VG Access             read/write
VG Status             resizable
MAX LV                0
Cur LV                0
Open LV               0
Max PV                0
Cur PV                3
Act PV                3
VG Size               &lt;2.99 GiB
PE Size               4.00 MiB
Total PE              765
Alloc PE / Size       0 / 0
Free  PE / Size       765 / &lt;2.99 GiB
VG UUID               ocl7ts-oYxV-SA8P-pgi0-gO4J-plT1-BKea7B
</pre></div>
</div>
</div>
</div>
<div class="section" id="create-logical-volume">
<h4><span class="section-number">13.5.1.3. </span>Create Logical Volume<a class="headerlink" href="#create-logical-volume" title="Permalink to this headline">¶</a></h4>
<p>Create a logical volume with 1+1 mirror (so 2 mirrors). Size is 40m for this test.</p>
<div class="section" id="id4">
<h5><span class="section-number">13.5.1.3.1. </span>Using the stack<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h5>
<p>Inside your hosts definition (here mngt1-1), add the lvm with mirror options:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">hosts</span><span class="p">:</span>
  <span class="nt">mngt1-1</span><span class="p">:</span>
    <span class="nt">storage</span><span class="p">:</span>
      <span class="nt">lvm</span><span class="p">:</span>
        <span class="p p-Indicator">-</span> <span class="nt">vg</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">vg1</span>
          <span class="nt">pvs</span><span class="p">:</span>
            <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">/dev/sdc1</span>
            <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">/dev/sdb1</span>
            <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">/dev/sdd1</span>
          <span class="nt">lvs</span><span class="p">:</span>
            <span class="p p-Indicator">-</span> <span class="nt">lv</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">TEST</span>
              <span class="nt">size</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">40m</span>
              <span class="nt">opts</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">-m 1</span>
</pre></div>
</div>
<p>And execute the <em>lvm</em> role on this host.</p>
</div>
<div class="section" id="id5">
<h5><span class="section-number">13.5.1.3.2. </span>Manually<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h5>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[root@mngt1-1 ~]# lvcreate -L +40m -m 1 -n TEST vg1
Logical volume &quot;TEST&quot; created.

[root@mngt1-1 ~]# lvdisplay
--- Logical volume ---
LV Path                /dev/vg1/TEST
LV Name                TEST
VG Name                vg1
LV UUID                hprEYi-VsHr-xaPU-ZwnF-vzdT-cTnb-x3evzx
LV Write Access        read/write
LV Creation host, time mngt1-1
LV Status              available
# open                 0
LV Size                40.00 MiB
Current LE             10
Mirrored volumes       2
Segments               1
Allocation             inherit
Read ahead sectors     auto
- currently set to     8192
Block device           253:4
</pre></div>
</div>
</div>
</div>
<div class="section" id="format-and-mount-the-volume">
<h4><span class="section-number">13.5.1.4. </span>Format and mount the volume<a class="headerlink" href="#format-and-mount-the-volume" title="Permalink to this headline">¶</a></h4>
<p>Now format it, in ext4:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[root@mngt1-1 ~]# mkfs.ext4 /dev/vg1/TEST
mke2fs 1.42.9 (28-Dec-2013)
Filesystem label=
OS type: Linux
Block size=1024 (log=0)
Fragment size=1024 (log=0)
Stride=0 blocks, Stripe width=0 blocks
10240 inodes, 40960 blocks
2048 blocks (5.00%) reserved for the super user
First data block=1
Maximum filesystem blocks=33685504
5 block groups
8192 blocks per group, 8192 fragments per group
2048 inodes per group
Superblock backups stored on blocks:
      8193, 24577

Allocating group tables: done
Writing inode tables: done

Creating journal (4096 blocks): done
Writing superblocks and filesystem accounting information: done

[root@mngt1-1 ~]# mount /dev/vg1/TEST /mnt
[root@mngt1-1 ~]# df
Filesystem           1K-blocks    Used Available Use% Mounted on
/dev/sda2             10189076 5241004   4407452  55% /
devtmpfs                497160       0    497160   0% /dev
tmpfs                   507752      24    507728   1% /dev/shm
tmpfs                   507752    6908    500844   2% /run
tmpfs                   507752       0    507752   0% /sys/fs/cgroup
/dev/sda1              1998672  108912   1768520   6% /boot
tmpfs                   101552       0    101552   0% /run/user/0
/dev/mapper/vg1-TEST     35567     782     31918   3% /mnt
[root@mngt1-1 ~]#
</pre></div>
</div>
<p>Copy a file for testing:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[root@mngt1-1 mnt]# cp /root/perl-Crypt-DES-2.05-20.el7.x86_64.rpm .
[root@mngt1-1 mnt]# ls
lost+found  perl-Crypt-DES-2.05-20.el7.x86_64.rpm
[root@mngt1-1 mnt]# echo &quot;Ho ! What can I do for you?&quot; &gt; blacksmith
[root@mngt1-1 mnt]# ls
blacksmith  lost+found  perl-Crypt-DES-2.05-20.el7.x86_64.rpm
[root@mngt1-1 mnt]# cat blacksmith
Ho ! What can I do for you?
[root@mngt1-1 mnt]# md5sum perl-Crypt-DES-2.05-20.el7.x86_64.rpm
f7457985634028c28b216c0b2145ecb0  perl-Crypt-DES-2.05-20.el7.x86_64.rpm
[root@mngt1-1 mnt]# umount /mnt
</pre></div>
</div>
<p>Mirrored volume is working fine.</p>
</div>
<div class="section" id="add-a-mirror">
<h4><span class="section-number">13.5.1.5. </span>Add a mirror<a class="headerlink" href="#add-a-mirror" title="Permalink to this headline">¶</a></h4>
<p>Most of the time, system administrators wish to add more mirrors.
To add a third mirror (because in the previous example there was
3 physical volumes, so 3 mirrors are possible):</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[root@mngt1-1 ~]# lvconvert -m 2 /dev/mapper/vg1-TEST
Are you sure you want to convert raid1 LV vg1/TEST to 3 images enhancing resilience? [y/n]: y
  Logical volume vg1/TEST successfully converted.
</pre></div>
</div>
<p>And check now a third mirror is ready. If volume is large,
synchronization in bellow table may take some time.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[root@mngt1-1 ~]# lvs -a -o +devices
  LV              VG  Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert Devices
  TEST            vg1 rwi-a-r--- 40.00m                                    100.00           TEST_rimage_0(0),TEST_rimage_1(0),TEST_rimage_2(0)
  [TEST_rimage_0] vg1 iwi-aor--- 40.00m                                                     /dev/sdb1(1)
  [TEST_rimage_1] vg1 iwi-aor--- 40.00m                                                     /dev/sdc1(1)
  [TEST_rimage_2] vg1 iwi-aor--- 40.00m                                                     /dev/sdd1(1)
  [TEST_rmeta_0]  vg1 ewi-aor---  4.00m                                                     /dev/sdb1(0)
  [TEST_rmeta_1]  vg1 ewi-aor---  4.00m                                                     /dev/sdc1(0)
  [TEST_rmeta_2]  vg1 ewi-aor---  4.00m                                                     /dev/sdd1(0)
[root@mngt1-1 ~]#
</pre></div>
</div>
</div>
<div class="section" id="recover-from-crash">
<h4><span class="section-number">13.5.1.6. </span>Recover from crash<a class="headerlink" href="#recover-from-crash" title="Permalink to this headline">¶</a></h4>
<p>In this example, we crashed one disk of the server.
Now system is unhealthy.</p>
<p>Check status:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[root@mngt1-1 ~]# lvdisplay -v /dev/mapper/vg1-TEST
WARNING: Device for PV wc7uAA-VCMc-uL2P-oQv1-o1kd-uxz1-eQWk0v not found or rejected by a filter.
  There are 1 physical volumes missing.
--- Logical volume ---
LV Path                /dev/vg1/TEST
LV Name                TEST
VG Name                vg1
LV UUID                hprEYi-VsHr-xaPU-ZwnF-vzdT-cTnb-x3evzx
LV Write Access        read/write
LV Creation host, time mngt1-1
LV Status              NOT available
LV Size                40.00 MiB
Current LE             10
Mirrored volumes       3
Segments               1
Allocation             inherit
Read ahead sectors     auto

[root@mngt1-1 ~]#
</pre></div>
</div>
<p>Note that if system rebooted (like here), LV is set as NOT available.</p>
<p>Add a new disk, and start again:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[root@mngt1-1 ~]#  lvdisplay
WARNING: Device for PV wc7uAA-VCMc-uL2P-oQv1-o1kd-uxz1-eQWk0v not found or rejected by a filter.
--- Logical volume ---
LV Path                /dev/vg1/TEST
LV Name                TEST
VG Name                vg1
LV UUID                hprEYi-VsHr-xaPU-ZwnF-vzdT-cTnb-x3evzx
LV Write Access        read/write
LV Creation host, time mngt1-1
LV Status              NOT available
LV Size                40.00 MiB
Current LE             10
Mirrored volumes       3
Segments               1
Allocation             inherit
Read ahead sectors     auto

[root@mngt1-1 ~]#
</pre></div>
</div>
<p>Still Not Available, because we still haven’t configured the new disk.</p>
<p>Activate LV, to use it with only 2 mirrors:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[root@mngt1-1 ~]# lvchange -a y /dev/mapper/vg1-TEST
</pre></div>
</div>
<p>System is in production, with only 2 mirrors now.</p>
<p>Now create a new pv /dev/sdb1 with new third disk, and extend vg1:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[root@mngt1-1 ~]# vgextend vg1 /dev/sdb1
WARNING: Device for PV wc7uAA-VCMc-uL2P-oQv1-o1kd-uxz1-eQWk0v not found or rejected by a filter.
WARNING: Device for PV wc7uAA-VCMc-uL2P-oQv1-o1kd-uxz1-eQWk0v not found or rejected by a filter.
Volume group &quot;vg1&quot; successfully extended
[root@mngt1-1 ~]#
</pre></div>
</div>
<p>Now clean vg1, to remove missing pv (wc7uAA-VCMc-uL2P-oQv1-o1kd-uxz1-eQWk0v).</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[root@mngt1-1 ~]# vgreduce --removemissing vg1 --force
WARNING: Device for PV wc7uAA-VCMc-uL2P-oQv1-o1kd-uxz1-eQWk0v not found or rejected by a filter.
Wrote out consistent volume group vg1.
[root@mngt1-1 ~]#
</pre></div>
</div>
<p>And repair:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[root@mngt1-1 ~]# lvconvert -m 2 /dev/mapper/vg1-TEST /dev/sdb1 /dev/sdc1 /dev/sdd1
[root@mngt1-1 ~]# lvconvert --repair /dev/mapper/vg1-TEST
WARNING: Disabling lvmetad cache for repair command.
WARNING: Not using lvmetad because of repair.
Attempt to replace failed RAID images (requires full device resync)? [y/n]: y
Faulty devices in vg1/TEST successfully replaced.
</pre></div>
</div>
<p>And check synchronization (may take some time):</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[root@mngt1-1 ~]# lvs -a -o +devices
WARNING: Not using lvmetad because a repair command was run.
LV              VG  Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert Devices
TEST            vg1 rwi-a-r--- 40.00m                                    100.00           TEST_rimage_0(0),TEST_rimage_1(0),TEST_rimage_2(0)
[TEST_rimage_0] vg1 iwi-aor--- 40.00m                                                     /dev/sdb1(1)
[TEST_rimage_1] vg1 iwi-aor--- 40.00m                                                     /dev/sdc1(1)
[TEST_rimage_2] vg1 iwi-aor--- 40.00m                                                     /dev/sdd1(1)
[TEST_rmeta_0]  vg1 ewi-aor---  4.00m                                                     /dev/sdb1(0)
[TEST_rmeta_1]  vg1 ewi-aor---  4.00m                                                     /dev/sdc1(0)
[TEST_rmeta_2]  vg1 ewi-aor---  4.00m                                                     /dev/sdd1(0)
[root@mngt1-1 ~]#
</pre></div>
</div>
<p>Test data are ok:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[root@mngt1-1 ~]# mount /dev/mapper/vg1-TEST /mnt
[root@mngt1-1 ~]# cd /mnt
[root@mngt1-1 mnt]# ls
blacksmith  lost+found  perl-Crypt-DES-2.05-20.el7.x86_64.rpm
[root@mngt1-1 mnt]# cat blacksmith
Ho ! What can I do for you?
[root@mngt1-1 mnt]#
</pre></div>
</div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="roles.html" class="btn btn-neutral float-right" title="14. Roles list" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="containers.html" class="btn btn-neutral float-left" title="12. Containers" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2019, Benoît Leveugle, Johnny Keats.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>