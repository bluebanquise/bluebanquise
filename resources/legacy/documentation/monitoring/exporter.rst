Exporters
=========

Multiple exporters are available for Prometheus.
Most of them can be found here:
https://prometheus.io/docs/instrumenting/exporters/

We will go through some of them.

Node_exporter
-------------

The node_exporter is an official exporter from Prometheus, and can be found
here: https://github.com/prometheus/node_exporter

By default, the node_exporter runs under the port 9100.

To access the metrics, either do:

.. code-block:: text

  curl http://localhost:9100/metrics

or access it directly in a browser. However, using curl can be handy, because
you can grep the output, and do other nice things with it.

You should get something like this:

.. code-block:: text

  # HELP node_vmstat_pgpgin /proc/vmstat information field pgpgin.
  # TYPE node_vmstat_pgpgin untyped
  node_vmstat_pgpgin 2.25160698e+08
  # HELP node_vmstat_pgpgout /proc/vmstat information field pgpgout.
  # TYPE node_vmstat_pgpgout untyped
  node_vmstat_pgpgout 1.18421998e+09
  # HELP node_vmstat_pswpin /proc/vmstat information field pswpin.
  # TYPE node_vmstat_pswpin untyped
  node_vmstat_pswpin 47719
  # HELP node_vmstat_pswpout /proc/vmstat information field pswpout.
  # TYPE node_vmstat_pswpout untyped
  node_vmstat_pswpout 532036
  # HELP process_cpu_seconds_total Total user and system CPU time spent in seconds.
  # TYPE process_cpu_seconds_total counter
  process_cpu_seconds_total 20272.88
  # HELP process_max_fds Maximum number of open file descriptors.
  # TYPE process_max_fds gauge
  process_max_fds 1024

There are a lot of information given by node_exporter. It is an heavy
exporter, that is generally not run on compute nodes, in order to prevent
any loss of computational power.

Some of the information given are the following:

* Cpu core throttle
* Cpu max,min,scaled frequency
* Cpu time spent in each mode
* Discards total time
* Disk information (discard time,discarded sectors,discards completed, discards merged,disk io, disk io time, disk io time weighted seconds, disk read bytes, disk read time,…)
* Filesystem (avail bytes, errors, mount points,etc…)
* Temperature sensors values
* Infiniband
* Memory usage, etc…
* Network( mtu bytes, protocols, received bytes, etc…)
* Nfs requests
* Scrape
* Etc…

You can see more by looking at the local metrics.

Alerts
^^^^^^

All the alerts for the node_exporter are stored under
/etc/prometheus/alerts/node.yml

Some of them include :

* High RAM usage
* High CPU usage
* High mount volume
* Host out of inodes
* Unusual disk write latency
* etc...

Start service
^^^^^^^^^^^^^

To start the service, simply run:

.. code-block:: text

  systemctl start node_exporter

.. note:: all exporter services are under the /etc/systemd/system directory, and most binaries are under the /usr/local/bin directory

Dashboard
^^^^^^^^^

Grafana open source dashboard:

* General info (structure, release, system, version,domain name)
* CPU Busy
* Used RAM, Used Max Mount, CPU IO wait, CPU Cores, etc..
* Mounts (Available, used, etc...)
* System load
* CPU Basic

To access the dashboard: access the management at http://localhost:3000

You can also do some "port forwarding" to reach Grafana.

Ipmi_exporter
-------------

The ipmi_exporter is an official Prometheus exporter, and can be found here:
https://github.com/soundcloud/ipmi_exporter

By default, the ipmi_exporter runs under the port 9290.

To access the metrics, either do:

.. code-block:: text

  curl 'http://localhost:9290/ipmi?target=mgmt1-1-bmc&module=equipment_R_E4m'

.. note::

  here, the request is http://<ip address of where the exporter is located>/snmp?target=<name of the taget to get metrics from>&module=<module>

.. note::

  you can get the list of all modules here: */etc/ipmi_exporter/ipmi_config.yml*

or you can access it directly in a browser. However, using curl can be handy,
because you can grep the output, and do other nice things with it.

You can get a list of all modules under */etc/ipmi_exporter/ipmi_config.yml*

The ipmi_config file is automatically generated by the ansible prometheus_server
role.

You should get something like this:

.. code-block:: text

  # HELP ipmi_bmc_info Constant metric with value '1' providing details about the BMC.
  # TYPE ipmi_bmc_info gauge
  ipmi_bmc_info{firmware_revision="3.70",manufacturer_id="Super Micro Computer Inc. (10876)"} 1
  # HELP ipmi_chassis_power_state Current power state (1=on, 0=off).
  # TYPE ipmi_chassis_power_state gauge
  ipmi_chassis_power_state 1
  # HELP ipmi_fan_speed_rpm Fan speed in rotations per minute.
  # TYPE ipmi_fan_speed_rpm gauge
  ipmi_fan_speed_rpm{id="1679",name="P-FAN1"} 7000
  ipmi_fan_speed_rpm{id="1746",name="P-FAN2"} 6600
  ipmi_fan_speed_rpm{id="1813",name="S-FAN1"} 7300
  ipmi_fan_speed_rpm{id="1880",name="S-FAN2"} 7500
  ipmi_fan_speed_rpm{id="1947",name="MB-FAN4"} 7400
  ipmi_fan_speed_rpm{id="2014",name="MB-FAN5"} 7100
  # HELP ipmi_fan_speed_state Reported state of a fan speed sensor (0=nominal, 1=warning, 2=critical).
  # TYPE ipmi_fan_speed_state gauge
  ipmi_fan_speed_state{id="1679",name="P-FAN1"} 0

Some of the information given are the following:

* Fan speed
* Power consumption in watts
* Temperature (multiple sensors)
* Voltage
* Amperes
* Etc…

You can see more by looking at the local metrics.

Alerts
^^^^^^

All the alerts for the ipmi_exporter are stored under /etc/prometheus/alerts/ipmi.yml

Some of them include :

* High fan speed
* Fan speed too high
* High power consumption
* Power consumption too high
* Problem with powers supply
* High ipmi temperature
* Ipmi temperature too high
* Scraping problem
* High voltage
* Voltage too high

Start service
^^^^^^^^^^^^^

To start the service, simply type:

.. code-block:: text

  systemctl start ipmi_exporter

You can change the BMC options of the inventory groups (compute nodes or
management nodes) under */etc/ipmi_exporter/ipmi_config.yml*

By default, it should look like this:

.. code-block:: yaml

  modules:

  equipment_compute_C:
    user: user
    pass: password
    driver: "LAN_2_0"
    privilege: "user"
    timeout: 10000
    collectors:
    - bmc
    - ipmi
    - chassis
    exclude_sensor_ids:


  equipment_R_E4m:
    user: ADMIN
    pass: ADMIN
    driver: "LAN_2_0"
    privilege: "user"
    timeout: 10000
    collectors:
    - bmc
    - ipmi
    - chassis
    exclude_sensor_ids:

If you modify the BMC username or password, don't forget to check the changes
in this file.

Dashboard
^^^^^^^^^

There are several dashboards for ipmi.

They give the following:

* Fan speed (min,max,avg,current) graph
* Temperature ( per sensors)
* Average Temperature of all sensors (min,max,avg,current)
* Alerts (warnings and critical)
* Power consumption (min,max,avg,current)
* Voltage (per sensors)
* Amperes
* etc...

Snmp_exporter
-------------

The snmp_exporter can be found here:
https://github.com/prometheus/snmp_exporter

By default, the snmp_exorter runs under the port 9116.

This exporter, along with the ipmi exporter, is a little special, as it executes
snmp commands on the targets. So we can get the metrics of all the targets with
the exporter running locally on our management server.

To access the metrics, do:

  curl 'http://localhost:9116/snmp?target=switch-001'

.. note::

  here, the request is http://<ip address of where the exporter is located>/snmp?target=<ip address of the switch that we want the metrics from>

Otherwise we can  access it directly in a browser. However, using curl can be
handy, because you can grep the output, and do other nice things with it.

You should get something like this:

.. code-block:: text

  # HELP ifAdminStatus The desired state of the interface - 1.3.6.1.2.1.2.2.1.7
  # TYPE ifAdminStatus gauge
  ifAdminStatus{ifAlias="",ifDescr="",ifIndex="1",ifName="Gi0/0"} 2
  ifAdminStatus{ifAlias="",ifDescr="",ifIndex="10",ifName="Gi1/0/3"} 1
  ifAdminStatus{ifAlias="",ifDescr="",ifIndex="11",ifName="Gi1/0/4"} 1
  ifAdminStatus{ifAlias="",ifDescr="",ifIndex="12",ifName="Gi1/0/5"} 1
  ifAdminStatus{ifAlias="",ifDescr="",ifIndex="13",ifName="Gi1/0/6"} 1
  ifAdminStatus{ifAlias="",ifDescr="",ifIndex="2",ifName="Nu0"} 1
  ifAdminStatus{ifAlias="",ifDescr="",ifIndex="20",ifName="Gi1/0/13"} 1
  ifAdminStatus{ifAlias="",ifDescr="",ifIndex="27",ifName="Gi1/0/20"} 1
  ifAdminStatus{ifAlias="",ifDescr="",ifIndex="29",ifName="Gi1/0/22"} 1
  ifAdminStatus{ifAlias="",ifDescr="",ifIndex="3",ifName="VLAN-1"} 1

Snmp_exporter setup
^^^^^^^^^^^^^^^^^^^

The setup of this exporter is a little tricky.

By default, we provide a configuration file for this exporter only for cisco
switches.

This is because snmp needs specific OIDS, in order to query the switches.
These OIDS can vary depending on the switch you use.

You can have a look at all OIDS available here:
https://cric.grenoble.cnrs.fr/Administrateurs/Outils/MIBS/

You can find this file under /etc/snmp_exporter/snmp.yml

It should look something like this::

.. code-block:: yaml

  if_mib:
    walk:
      - 1.3.6.1.2.1.2
      - 1.3.6.1.2.1.31.1.1
      - 1.3.6.1.4.1.25461.2.1.2.3.11.1.2
  get:
    - 1.3.6.1.2.1.1.3.0
  metrics:
    - name: sysUpTime
      oid: 1.3.6.1.2.1.1.3
      type: gauge
      help: The time (in hundredths of a second) since the network management portion of the system was last re-initialized. - 1.3.6.1.2.1.1.3
    - name: ifNumber
      oid: 1.3.6.1.2.1.2.1
      type: gauge
      help: The number of network interfaces (regardless of their current state) present on this system. - 1.3.6.1.2.1.2.1
    - name: ifIndex
      oid: 1.3.6.1.2.1.2.2.1.1
      type: gauge
      help: A unique value, greater than zero, for each interface - 1.3.6.1.2.1.2.2.1.1
      indexes:

If you have another switch you want to query, you can generate another file than
the one we provide.

By installing snmp_exporter, you should have a generator installed under
/usr/local/go/src/github.com/prometheus/snmp_exporter/generator

Here, you have a file, generator.yml that you have to change, according to what
you want.

By default, it looks like this:

.. code-block:: text

  modules:
  # Default IF-MIB interfaces table with ifIndex.
  if_mib:
    walk: [sysUpTime, interfaces,ifXTable]
    version: 1
    auth:
      community: cluster
    lookups:
      - source_indexes: [ifIndex]
        lookup: ifAlias
      - source_indexes: [ifIndex]
        lookup: ifDescr
      - source_indexes: [ifIndex]
        # Use OID to avoid conflict with Netscaler NS-ROOT-MIB.
        lookup: 1.3.6.1.2.1.31.1.1.1.1 # ifName
    overrides:
      ifAlias:
        ignore: true # Lookup metric
      ifDescr:
        ignore: true # Lookup metric
      ifName:
        ignore: true # Lookup metric
      ifType:
        type: EnumAsInfo

.. note:: Notice the auth section, by default, we setup the switches with the cluster community with no password required. See the switch setup section for more info.

You can tune it as you want, as long as you follow this syntax:

.. code-block:: text

  modules:
  module_name:  # The module name. You can have as many modules as you want.
    walk:       # List of OIDs to walk. Can also be SNMP object names or specific instances.
      - 1.3.6.1.2.1.2              # Same as "interfaces"
      - sysUpTime                  # Same as "1.3.6.1.2.1.1.3"
      - 1.3.6.1.2.1.31.1.1.1.6.40  # Instance of "ifHCInOctets" with index "40"

    version: 2  # SNMP version to use. Defaults to 2.
                # 1 will use GETNEXT, 2 and 3 use GETBULK.
    max_repetitions: 25  # How many objects to request with GET/GETBULK, defaults to 25.
                         # May need to be reduced for buggy devices.
    retries: 3   # How many times to retry a failed request, defaults to 3.
    timeout: 5s  # Timeout for each individual SNMP request, defaults to 5s.

    auth:
      # Community string is used with SNMP v1 and v2. Defaults to "public".
      community: public

      # v3 has different and more complex settings.
      # Which are required depends on the security_level.
      # The equivalent options on NetSNMP commands like snmpbulkwalk
      # and snmpget are also listed. See snmpcmd(1).
      username: user  # Required, no default. -u option to NetSNMP.
      security_level: noAuthNoPriv  # Defaults to noAuthNoPriv. -l option to NetSNMP.
                                    # Can be noAuthNoPriv, authNoPriv or authPriv.
      password: pass  # Has no default. Also known as authKey, -A option to NetSNMP.
                      # Required if security_level is authNoPriv or authPriv.
      auth_protocol: MD5  # MD5 or SHA, defaults to MD5. -a option to NetSNMP.
                          # Used if security_level is authNoPriv or authPriv.
      priv_protocol: DES  # DES or AES, defaults to DES. -x option to NetSNMP.
                          # Used if security_level is authPriv.
      priv_password: otherPass # Has no default. Also known as privKey, -X option to NetSNMP.
                               # Required if security_level is authPriv.
      context_name: context # Has no default. -n option to NetSNMP.
                            # Required if context is configured on the device.

    lookups:  # Optional list of lookups to perform.
              # The default for `keep_source_indexes` is false. Indexes must be unique for this option to be used.

      # If the index of a table is bsnDot11EssIndex, usually that'd be the label
      # on the resulting metrics from that table. Instead, use the index to
      # lookup the bsnDot11EssSsid table entry and create a bsnDot11EssSsid label
      # with that value.
      - source_indexes: [bsnDot11EssIndex]
        lookup: bsnDot11EssSsid
        drop_source_indexes: false  # If true, delete source index labels for this lookup.
                                    # This avoids label clutter when the new index is unique.

     overrides: # Allows for per-module overrides of bits of MIBs
       metricName:
         ignore: true # Drops the metric from the output.
         regex_extracts:
           Temp: # A new metric will be created appending this to the metricName to become metricNameTemp.
             - regex: '(.*)' # Regex to extract a value from the returned SNMP walks's value.
               value: '$1' # The result will be parsed as a float64, defaults to $1.
           Status:
             - regex: '.*Example'
               value: '1' # The first entry whose regex matches and whose value parses wins.
             - regex: '.*'
               value: '0'
         type: DisplayString # Override the metric type, possible types are:
                             #   gauge:   An integer with type gauge.
                             #   counter: An integer with type counter.
                             #   OctetString: A bit string, rendered as 0xff34.
                             #   DateAndTime: An RFC 2579 DateAndTime byte sequence. If the device has no time zone data, UTC is used.
                             #   DisplayString: An ASCII or UTF-8 string.
                             #   PhysAddress48: A 48 bit MAC address, rendered as 00:01:02:03:04:ff.
                             #   Float: A 32 bit floating-point value with type gauge.
                             #   Double: A 64 bit floating-point value with type gauge.
                             #   InetAddressIPv4: An IPv4 address, rendered as 1.2.3.4.
                             #   InetAddressIPv6: An IPv6 address, rendered as 0102:0304:0506:0708:090A:0B0C:0D0E:0F10.
                             #   InetAddress: An InetAddress per RFC 4001. Must be preceded by an InetAddressType.
                             #   InetAddressMissingSize: An InetAddress that violates section 4.1 of RFC 4001 by
                             #       not having the size in the index. Must be preceded by an InetAddressType.
                             #   EnumAsInfo: An enum for which a single timeseries is created. Good for constant values.
                             #   EnumAsStateSet: An enum with a time series per state. Good for variable low-cardinality enums.
                             #   Bits: An RFC 2578 BITS construct, which produces a StateSet with a time series per bit.


Here is a list of MIBS:

.. seealso:: https://github.com/librenms/librenms/tree/master/mibs

You can get more info here:

.. seealso:: https://github.com/prometheus/snmp_exporter/tree/master/generator

And here:

.. seealso:: https://programmer.group/prometheus-prometheus-monitoring-switch-snmp.html

Once you are done tuning the file, simply do:

.. code-block:: text

  $ export MIBDIRS=mibs
  $ ./generator generate

>hat you will get is a snmp.yml file. Simply copy the new file:

.. code-block:: text

  $ cp snmp.yml /etc/snmp_exporter/

Setup targets
^^^^^^^^^^^^^

To setup the targets, simply add:

.. code-block:: yaml

  monitoring:

  exporters:
    snmp_exporter:
      port: 9116
      with_generator: false

to the /etc/ansible/inventory/group_vars/equipment_profile you desire.

Switch setup
^^^^^^^^^^^^

To setup the community on the switch to communicate with the exporter:
Go to the switch via ssh or telnet, and enter the following commands:

.. code-block:: text

  $ Enable
  $ configure terminal
  $ snmp-server community cluster RO
  $ exit
  $ write memory

You can change cluster to any community name you want, that is written in the
 snmp.yml file

Start service
^^^^^^^^^^^^^

To start the service, simply run:

.. code-block:: text

  systemctl start snmp_exporter

.. note:: all exporter services are under the /etc/systemd/system directory, and most binaries are under the /usr/local/bin directory

Dashboard
^^^^^^^^^

The dashboard gives the following:

* Interface thoughput( in and out)
* Interface in,out,total in, total out, Bandwidth
* Alerts
* Percentage of casts (uni,multi,etc) In and Out
* Max in, Max out, number of interfaces, Total in,Uptime, Total out
* Etc...

Ha_cluster_exporter
-------------------

The ha_cluster_exporter is an open source exporter, and can be found here:
https://github.com/ClusterLabs/ha_cluster_exporter

.. note::
  No packages are provided in the stack for this exporter.
  You will need to build it yourself.

By default, the ha_cluster_exorter runs under the port 9664.

To access the metrics, either do:

.. code-block:: text

  curl http://localhost:9664/metrics

Or access it directly in a browser. However, using curl can be handy, because
you can grep the output, and do other nice things with it.

You should get something like this:

.. code-block:: text

  # HELP ha_cluster_corosync_member_votes How many votes each member node has contributed with to the current quorum
  # TYPE ha_cluster_corosync_member_votes gauge
  ha_cluster_corosync_member_votes{local="true",node="mgmt1-2",node_id="1"} 1
  # HELP ha_cluster_corosync_quorate Whether or not the cluster is quorate
  # TYPE ha_cluster_corosync_quorate gauge
  ha_cluster_corosync_quorate 1
  # HELP ha_cluster_corosync_quorum_votes Cluster quorum votes; one line per type
  # TYPE ha_cluster_corosync_quorum_votes gauge
  ha_cluster_corosync_quorum_votes{type="expected_votes"} 2
  ha_cluster_corosync_quorum_votes{type="highest_expected"} 2
  ha_cluster_corosync_quorum_votes{type="quorum"} 1
  ha_cluster_corosync_quorum_votes{type="total_votes"} 1
  # HELP ha_cluster_corosync_ring_errors The total number of faulty corosync rings
  # TYPE ha_cluster_corosync_ring_errors gauge
  ha_cluster_corosync_ring_errors 0
  # HELP ha_cluster_pacemaker_config_last_change The timestamp of the last change of the cluster configuration
  # TYPE ha_cluster_pacemaker_config_last_change counter
  ha_cluster_pacemaker_config_last_change 1.593617322e+09
  # HELP ha_cluster_pacemaker_fail_count The Fail count number per node and resource id
  # TYPE ha_cluster_pacemaker_fail_count gauge
  ha_cluster_pacemaker_fail_count{node="mgmt1-2",resource="fs-conman"} 0
  ha_cluster_pacemaker_fail_count{node="mgmt1-2",resource="fs-data-http"} 0
  ha_cluster_pacemaker_fail_count{node="mgmt1-2",resource="fs-data-pgsql"} 0
  ha_cluster_pacemaker_fail_count{node="mgmt1-2",resource="fs-grafana-database"} 0
  ha_cluster_pacemaker_fail_count{node="mgmt1-2",resource="fs-prometheus-database"} 0
  ha_cluster_pacemaker_fail_count{node="mgmt1-2",resource="fs-rsyslog"} 0


Some of the information given are the following:

* Pacemaker fails
* Pacemaker constraints
* Pacemaker migration threshold
* Pacemaker node status
* Status of Pacemaker resources
* Pacemaker stonith (enabled or not)

You can see more by looking at the local metrics, and by checking the github
page.

Alerts
^^^^^^

All the alerts for the node_exporter are stored under
/etc/prometheus/alerts/ha.yml

Some of them include :

* Not quorate
* Long standby
* Failed services
* Failed resources
* Failcount>migration threshold
* Stonith not enabled
* Negative location constraints

Start service
^^^^^^^^^^^^^

To start the service, simply run:

.. code-block:: text

  systemctl start ha_cluster_exporter

.. note::
  All exporter services are under the /etc/systemd/system directory,
  and most binaries are under the /usr/local/bin directory.

Dashboards
^^^^^^^^^^

there are several dashboards for the ha.

Here is what they show:

* Pacemaker nodes (Total  nodes, online nodes, expected up, DC)
* Quorum votes (expected votes, highest expected vote, total votes)
* Is quorate?
* Ring errors
* Last pacemaker change
* Resources (names, agent,status,…)
* Alerts (name, severity, instance…)
* Etc...

Slurm_exporter
--------------

The slurm_exporter is an open source exporter for Prometheus, and can be found
here: https://github.com/vpenso/prometheus-slurm-exporter

.. note::
  No packages are provided in the stack for this exporter.
  You will need to build it yourself.

By default, the slurm_exorter runs under the port 9817.

To access the metrics, either do::

  curl http://localhost:9817/metrics

or access it directly in a browser. However, using curl can be handy, because
you can grep the output, and do other nice things with it.

You should get something like this:

.. code-block:: text

  # TYPE promhttp_metric_handler_requests_total counter
  promhttp_metric_handler_requests_total{code="200"} 3029
  promhttp_metric_handler_requests_total{code="500"} 0
  promhttp_metric_handler_requests_total{code="503"} 0
  # HELP slurm_cpus_alloc Allocated CPUs
  # TYPE slurm_cpus_alloc gauge
  slurm_cpus_alloc 65792
  # HELP slurm_cpus_idle Idle CPUs
  # TYPE slurm_cpus_idle gauge
  slurm_cpus_idle 0
  # HELP slurm_cpus_other Mix CPUs
  # TYPE slurm_cpus_other gauge
  slurm_cpus_other 15616
  # HELP slurm_cpus_total Total CPUs
  # TYPE slurm_cpus_total gauge
  slurm_cpus_total 81408
  # HELP slurm_nodes_alloc Allocated nodes
  # TYPE slurm_nodes_alloc gauge
  slurm_nodes_alloc 257
  # HELP slurm_nodes_comp Completing nodes
  # TYPE slurm_nodes_comp gauge
  slurm_nodes_comp 0
  # HELP slurm_nodes_down Down nodes
  # TYPE slurm_nodes_down gauge
  slurm_nodes_down 28


Metrics
^^^^^^^

Here is an extract from the github page:

.. code-block:: text

  ## Exported Metrics

  ### State of the CPUs

  * **Allocated**: CPUs which have been allocated to a job.
  * **Idle**: CPUs not allocated to a job and thus available for use.
  * **Other**: CPUs which are unavailable for use at the moment.
  * **Total**: total number of CPUs.

  - [Information extracted from the SLURM **sinfo** command](https://slurm.schedmd.com/sinfo.html)
  - [Slurm CPU Management User and Administrator Guide](https://slurm.schedmd.com/cpu_management.html)

  ### State of the Nodes

  * **Allocated**: nodes which has been allocated to one or more jobs.
  * **Completing**: all jobs associated with these nodes are in the process of being completed.
  * **Down**: nodes which are unavailable for use.
  * **Drain**: with this metric two different states are accounted for:
    - nodes in ``drained`` state (marked unavailable for use per system administrator request)
    - nodes in ``draining`` state (currently executing jobs but which will not be allocated for new ones).
  * **Fail**: these nodes are expected to fail soon and are unavailable for use per system administrator request.
  * **Error**: nodes which are currently in an error state and not capable of running any jobs.
  * **Idle**: nodes not allocated to any jobs and thus available for use.
  * **Maint**: nodes which are currently marked with the __maintenance__ flag.
  * **Mixed**: nodes which have some of their CPUs ALLOCATED while others are IDLE.
  * **Resv**: these nodes are in an advanced reservation and not generally available.

  [Information extracted from the SLURM **sinfo** command](https://slurm.schedmd.com/sinfo.html)

  ### Status of the Jobs

  * **PENDING**: Jobs awaiting for resource allocation.
  * **PENDING_DEPENDENCY**: Jobs awaiting because of a unexecuted job dependency.
  * **RUNNING**: Jobs currently allocated.
  * **SUSPENDED**: Job has an allocation but execution has been suspended and CPUs have been released for other jobs.
  * **CANCELLED**: Jobs which were explicitly cancelled by the user or system administrator.
  * **COMPLETING**: Jobs which are in the process of being completed.
  * **COMPLETED**: Jobs have terminated all processes on all nodes with an exit code of zero.
  * **CONFIGURING**: Jobs have been allocated resources, but are waiting for them to become ready for use.
  * **FAILED**: Jobs terminated with a non-zero exit code or other failure condition.
  * **TIMEOUT**: Jobs terminated upon reaching their time limit.
  * **PREEMPTED**: Jobs terminated due to preemption.
  * **NODE_FAIL**: Jobs terminated due to failure of one or more allocated nodes.

  [Information extracted from the SLURM **squeue** command](https://slurm.schedmd.com/squeue.html)

  ### Scheduler Information

  * **Server Thread count**: The number of current active ``slurmctld`` threads.
  * **Queue size**: The length of the scheduler queue.
  * **Last cycle**: Time in microseconds for last scheduling cycle.
  * **Mean cycle**: Mean of scheduling cycles since last reset.
  * **Cycles per minute**: Counter of scheduling executions per minute.
  * **(Backfill) Last cycle**: Time in microseconds of last backfilling cycle.
  * **(Backfill) Mean cycle**: Mean of backfilling scheduling cycles in microseconds since last reset.
  * **(Backfill) Depth mean**: Mean of processed jobs during backfilling scheduling cycles since last reset.


You can see more by looking at the local metrics.

Start service
^^^^^^^^^^^^^

To start the service, simply run:

.. code-block:: text

  systemctl start slurm_exporter

.. note:: all exporter services are under the /etc/systemd/system directory, and most binaries are under the /usr/local/bin directory

Alerts
^^^^^^

All the alerts for the slurm_exporter are stored under /etc/prometheus/alerts/

Some of them include :

* High RAM usage
* High CPU usage
* High mount volume
* Host out of inodes
* Unusual disk write latency
* etc...

Dashboard
^^^^^^^^^

A dashboard is provided on the exporter github page.
