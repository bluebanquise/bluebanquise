#### Blue Banquise file ####
## {{ansible_managed}}

## Very minimal slurm.conf

## Controller
ClusterName={{HPC_slurm['cluster_name']}}
ControlMachine={{HPC_slurm['control_machine']}}

## Authentication
SlurmUser=slurm
AuthType=auth/munge
CryptoType=crypto/munge

## Files path
StateSaveLocation=/var/spool/slurmd/StateSave
SlurmdSpoolDir=/var/spool/slurmd/slurmd
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid

## Logging
SlurmctldDebug=5
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=5
SlurmdLogFile=/var/log/slurm/slurmd.log

## Timeouts
SlurmctldTimeout=600
SlurmdTimeout=600

## We don't want a node to go back in pool without sys admin acknoledgement
ReturnToService=0

## If you are using pmi2 interface for MPI
#MpiDefault=pmi2

## Basic scheduling based on CPU
SelectTypeParameters=CR_Core_Memory,CR_CORE_DEFAULT_DIST_BLOCK
SchedulerType=sched/backfill
SelectType=select/cons_res


## Nodes list
{% for equipment_group in HPC_slurm['nodes_equipment_groups'] %}
{% if groups[equipment_group] is defined and not none %}
{% for node in groups[equipment_group] %}
NodeName={{node}} Procs={{hostvars[node]['equipment_profile']['hardware']['cpu']['cores']}} State=UNKNOWN
{% endfor %}
{% endif %}
{% endfor %}

## Partitions list
{% for equipment_group in HPC_slurm['nodes_equipment_groups'] %}
{% if groups[equipment_group] is defined and not none %}
PartitionName={{equipment_group}} MaxTime=INFINITE State=UP Nodes={% for node in groups[equipment_group] %}{% if loop.first %}{{node}}{% else %},{{node}}{% endif %}{% endfor %}

{% endif %}
{% endfor %}
